{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2ae8652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best policy for each opponent type and discount factor.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from IPython.display import Video\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our custom environment\n",
    "# Use importlib to ensure we get the latest version (clears cache)\n",
    "import importlib\n",
    "import prisoners_dilemma_env\n",
    "importlib.reload(prisoners_dilemma_env)\n",
    "\n",
    "from prisoners_dilemma_env import IteratedPrisonersDilemma, COOPERATE, DEFECT, ACTION_MAP\n",
    "\n",
    "# Import utility functions\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "from utils import print_policy, print_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59906ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-C\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "\n",
      "\n",
      "Random vs Best Policy:\n",
      "  Random: -390.00\n",
      "  Best:   -300.00\n",
      "  Diff:   90.00\n",
      "\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-D\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "\n",
      "\n",
      "Random vs Best Policy:\n",
      "  Random: -56.00\n",
      "  Best:   0.00\n",
      "  Diff:   56.00\n",
      "\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: TFT\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "(C, D)          1          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -10.0000  \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -10.0000  \n",
      "\n",
      "\n",
      "Random vs Best Policy:\n",
      "  Random: -220.00\n",
      "  Best:   -104.00\n",
      "  Diff:   116.00\n",
      "\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: IMPERFECT-TFT\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          0.0000     1.0000     Defect (D)      -17.2000  \n",
      "(C, D)          1          0.0000     1.0000     Defect (D)      -17.2000  \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "\n",
      "\n",
      "Random vs Best Policy:\n",
      "  Random: -239.00\n",
      "  Best:   -140.00\n",
      "  Diff:   99.00\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# END OF EXPERIMENT\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from operator import ge\n",
    "\n",
    "strategies = [\"ALL-C\", \"ALL-D\", \"TFT\", \"IMPERFECT-TFT\"]\n",
    "memory_schemes = [1, 2]\n",
    "gamma = 0.9\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT: Policy Iteration for Different Opponent Strategies\n",
    "# ============================================================================\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Opponent: ALL-C (Always Cooperate)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "env = IteratedPrisonersDilemma(opponent_strategy=\"ALL-C\", memory_scheme=1)\n",
    "best_policy, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
    "print_policy(best_policy, value_function, env, opponent_strategy=\"ALL-C\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# COMPARISON: Random Policy vs Best Policy for ALL-C\n",
    "# ----------------------------------------------------------------------------\n",
    "comparison = env.test_against_random(best_policy, num_steps=100)\n",
    "print_comparison(comparison)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Opponent: ALL-D (Always Defect)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "env = IteratedPrisonersDilemma(opponent_strategy=\"ALL-D\", memory_scheme=1)\n",
    "best_policy, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
    "print_policy(best_policy, value_function, env, opponent_strategy=\"ALL-D\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# COMPARISON: Random Policy vs Best Policy for ALL-D\n",
    "# ----------------------------------------------------------------------------\n",
    "comparison = env.test_against_random(best_policy, num_steps=100)\n",
    "print_comparison(comparison)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Opponent: TFT (Tit-for-Tat)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "env = IteratedPrisonersDilemma(opponent_strategy=\"TFT\", memory_scheme=1)\n",
    "best_policy, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
    "print_policy(best_policy, value_function, env, opponent_strategy=\"TFT\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# COMPARISON: Random Policy vs Best Policy for TFT\n",
    "# ----------------------------------------------------------------------------\n",
    "comparison = env.test_against_random(best_policy, num_steps=100)\n",
    "print_comparison(comparison)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Opponent: IMPERFECT-TFT (Imperfect Tit-for-Tat)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "env = IteratedPrisonersDilemma(opponent_strategy=\"IMPERFECT-TFT\", memory_scheme=1)\n",
    "best_policy, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
    "print_policy(best_policy, value_function, env, opponent_strategy=\"IMPERFECT-TFT\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# COMPARISON: Random Policy vs Best Policy for IMPERFECT-TFT\n",
    "# ----------------------------------------------------------------------------\n",
    "comparison = env.test_against_random(best_policy, num_steps=100)\n",
    "print_comparison(comparison)\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# END OF EXPERIMENT\")\n",
    "print(\"#\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae73ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT: Policy Iteration for Different Discount Factors (Gamma)\n",
    "# ============================================================================\n",
    "# Define gamma values to test\n",
    "gamma_values = [0.1, 0.5, 0.9, 0.99]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1966ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\n",
      "################################################################################\n",
      "# Opponent Strategy: ALL-C (Always Cooperate)\n",
      "# Memory Scheme: 1\n",
      "################################################################################\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-C\n",
      "Discount Factor (γ): 0.1\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -3.3333   \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -3.3333   \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   -3.3333   \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   -3.3333   \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-C\n",
      "Discount Factor (γ): 0.5\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -6.0000   \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -6.0000   \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   -6.0000   \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   -6.0000   \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.9\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-C\n",
      "Discount Factor (γ): 0.9\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.99\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-C\n",
      "Discount Factor (γ): 0.99\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -299.9999 \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -299.9999 \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   -299.9999 \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   -299.9999 \n",
      "\n",
      "\n",
      "################################################################################\n",
      "# END OF EXPERIMENT - ALL-C\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPPONENT: ALL-C (Always Cooperate) - Different Discount Factors\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    env = IteratedPrisonersDilemma(opponent_strategy=\"ALL-C\", memory_scheme=1)\n",
    "    best_policy, value_function = env.policy_iteration(gamma=gamma, theta=0.000001, max_iterations=100)\n",
    "    print_policy(best_policy, value_function, env, opponent_strategy=\"ALL-C\", gamma=gamma)\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# END OF EXPERIMENT - ALL-C\")\n",
    "print(\"#\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "90eeb220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\n",
      "################################################################################\n",
      "# Opponent Strategy: ALL-D (Always Defect)\n",
      "# Memory Scheme: 1\n",
      "################################################################################\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-D\n",
      "Discount Factor (γ): 0.1\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-D\n",
      "Discount Factor (γ): 0.5\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.9\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-D\n",
      "Discount Factor (γ): 0.9\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.99\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-D\n",
      "Discount Factor (γ): 0.99\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "\n",
      "\n",
      "################################################################################\n",
      "# END OF EXPERIMENT - ALL-D\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPPONENT: ALL-D (Always Defect) - Different Discount Factors\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\")\n",
    "print(\"#\" * 80)\n",
    "print(\"# Opponent Strategy: ALL-D (Always Defect)\")\n",
    "print(\"# Memory Scheme: 1\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(f\"DISCOUNT FACTOR (γ): {gamma}\")\n",
    "    print(\"-\" * 80)\n",
    "    env = IteratedPrisonersDilemma(opponent_strategy=\"ALL-D\", memory_scheme=1)\n",
    "    best_policy, value_function = env.policy_iteration(gamma=gamma, theta=0.000001, max_iterations=100)\n",
    "    print_policy(best_policy, value_function, env, opponent_strategy=\"ALL-D\", gamma=gamma)\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# END OF EXPERIMENT - ALL-D\")\n",
    "print(\"#\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2011e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\n",
      "################################################################################\n",
      "# Opponent Strategy: TFT (Tit-for-Tat)\n",
      "# Memory Scheme: 1\n",
      "################################################################################\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: TFT\n",
      "Discount Factor (γ): 0.1\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -3.3333   \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -3.3333   \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   -0.3333   \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   -0.3333   \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: TFT\n",
      "Discount Factor (γ): 0.5\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -6.0000   \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -6.0000   \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -2.0000   \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -2.0000   \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.9\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: TFT\n",
      "Discount Factor (γ): 0.9\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "(C, D)          1          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -10.0000  \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -10.0000  \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.99\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: TFT\n",
      "Discount Factor (γ): 0.99\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          0.0000     1.0000     Defect (D)      -103.9999 \n",
      "(C, D)          1          0.0000     1.0000     Defect (D)      -103.9999 \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -99.9999  \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -99.9999  \n",
      "\n",
      "\n",
      "################################################################################\n",
      "# END OF EXPERIMENT - TFT\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPPONENT: TFT (Tit-for-Tat) - Different Discount Factors\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\")\n",
    "print(\"#\" * 80)\n",
    "print(\"# Opponent Strategy: TFT (Tit-for-Tat)\")\n",
    "print(\"# Memory Scheme: 1\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(f\"DISCOUNT FACTOR (γ): {gamma}\")\n",
    "    print(\"-\" * 80)\n",
    "    env = IteratedPrisonersDilemma(opponent_strategy=\"TFT\", memory_scheme=1)\n",
    "    best_policy, value_function = env.policy_iteration(gamma=gamma, theta=0.000001, max_iterations=100)\n",
    "    print_policy(best_policy, value_function, env, opponent_strategy=\"TFT\", gamma=gamma)\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# END OF EXPERIMENT - TFT\")\n",
    "print(\"#\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "858ca6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\n",
      "################################################################################\n",
      "# Opponent Strategy: IMPERFECT-TFT (Imperfect Tit-for-Tat)\n",
      "# Memory Scheme: 1\n",
      "################################################################################\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: IMPERFECT-TFT\n",
      "Discount Factor (γ): 0.1\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -3.0000   \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -3.0000   \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   -0.6000   \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   -0.6000   \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: IMPERFECT-TFT\n",
      "Discount Factor (γ): 0.5\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -5.4000   \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -5.4000   \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -2.8000   \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -2.8000   \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.9\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: IMPERFECT-TFT\n",
      "Discount Factor (γ): 0.9\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          0.0000     1.0000     Defect (D)      -17.2000  \n",
      "(C, D)          1          0.0000     1.0000     Defect (D)      -17.2000  \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.99\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: IMPERFECT-TFT\n",
      "Discount Factor (γ): 0.99\n",
      "Memory Scheme: 1\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          0.0000     1.0000     Defect (D)      -143.1999 \n",
      "(C, D)          1          0.0000     1.0000     Defect (D)      -143.1999 \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -139.9999 \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -139.9999 \n",
      "\n",
      "\n",
      "################################################################################\n",
      "# END OF EXPERIMENT - IMPERFECT-TFT\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPPONENT: IMPERFECT-TFT (Imperfect Tit-for-Tat) - Different Discount Factors\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\")\n",
    "print(\"#\" * 80)\n",
    "print(\"# Opponent Strategy: IMPERFECT-TFT (Imperfect Tit-for-Tat)\")\n",
    "print(\"# Memory Scheme: 1\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(f\"DISCOUNT FACTOR (γ): {gamma}\")\n",
    "    print(\"-\" * 80)\n",
    "    env = IteratedPrisonersDilemma(opponent_strategy=\"IMPERFECT-TFT\", memory_scheme=1)\n",
    "    best_policy, value_function = env.policy_iteration(gamma=gamma, theta=0.000001, max_iterations=100)\n",
    "    print_policy(best_policy, value_function, env, opponent_strategy=\"IMPERFECT-TFT\", gamma=gamma)\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# END OF EXPERIMENT - IMPERFECT-TFT\")\n",
    "print(\"#\" * 80 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
