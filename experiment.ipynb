{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "402d89db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy, value_function, env, opponent_strategy=None, gamma=None):\n",
    "    \"\"\"\n",
    "    Print policy and value function in a readable format.\n",
    "    \n",
    "    Args:\n",
    "        policy: Policy matrix of shape (num_states, num_actions)\n",
    "        value_function: Value function array of shape (num_states,)\n",
    "        env: IteratedPrisonersDilemma environment instance\n",
    "        opponent_strategy: Optional string to display opponent strategy name\n",
    "        gamma: Optional discount factor to display\n",
    "    \"\"\"\n",
    "    num_states = env.observation_space.n\n",
    "    memory_scheme = env.memory_scheme\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"OPTIMAL POLICY AND VALUE FUNCTION\")\n",
    "    print(\"=\" * 80)\n",
    "    if opponent_strategy:\n",
    "        print(f\"Opponent Strategy: {opponent_strategy}\")\n",
    "    if gamma is not None:\n",
    "        print(f\"Discount Factor (γ): {gamma}\")\n",
    "    print(f\"Memory Scheme: {memory_scheme}\")\n",
    "    print(f\"Number of States: {num_states}\")\n",
    "    print()\n",
    "    \n",
    "    # Print policy table\n",
    "    print(\"POLICY (π):\")\n",
    "    print(\"-\" * 80)\n",
    "    if memory_scheme == 1:\n",
    "        # Memory-1: 4 states\n",
    "        state_names = [\"(C, C)\", \"(C, D)\", \"(D, C)\", \"(D, D)\"]\n",
    "        print(f\"{'State':<15} {'State ID':<10} {'P(C)':<10} {'P(D)':<10} {'Best Action':<15} {'V(s)':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        for s in range(num_states):\n",
    "            state_name = state_names[s]\n",
    "            p_cooperate = policy[s, 0]\n",
    "            p_defect = policy[s, 1]\n",
    "            best_action_idx = np.argmax(policy[s])\n",
    "            best_action = \"Cooperate (C)\" if best_action_idx == 0 else \"Defect (D)\"\n",
    "            v_value = value_function[s]\n",
    "            print(f\"{state_name:<15} {s:<10} {p_cooperate:<10.4f} {p_defect:<10.4f} {best_action:<15} {v_value:<10.4f}\")\n",
    "    else:\n",
    "        # Memory-2: 16 states\n",
    "        print(f\"{'State':<25} {'State ID':<10} {'P(C)':<10} {'P(D)':<10} {'Best Action':<15} {'V(s)':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        for s in range(num_states):\n",
    "            # Decode state: [A_t-1, O_t-1, A_t-2, O_t-2]\n",
    "            state_vector = []\n",
    "            temp = s\n",
    "            for i in range(4):\n",
    "                bit = temp % 2\n",
    "                state_vector.insert(0, bit)\n",
    "                temp = temp // 2\n",
    "            A_t1, O_t1, A_t2, O_t2 = state_vector\n",
    "            \n",
    "            # Format state name\n",
    "            action_map = {0: \"C\", 1: \"D\"}\n",
    "            state_name = f\"({action_map[A_t1]},{action_map[O_t1]})→({action_map[A_t2]},{action_map[O_t2]})\"\n",
    "            \n",
    "            p_cooperate = policy[s, 0]\n",
    "            p_defect = policy[s, 1]\n",
    "            best_action_idx = np.argmax(policy[s])\n",
    "            best_action = \"Cooperate (C)\" if best_action_idx == 0 else \"Defect (D)\"\n",
    "            v_value = value_function[s]\n",
    "            print(f\"{state_name:<25} {s:<10} {p_cooperate:<10.4f} {p_defect:<10.4f} {best_action:<15} {v_value:<10.4f}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Average State Value: {np.mean(value_function):.4f}\")\n",
    "    print(f\"Max State Value: {np.max(value_function):.4f}\")\n",
    "    print(f\"Min State Value: {np.min(value_function):.4f}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2ae8652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best policy for each opponent type and discount factor.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bad8d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from IPython.display import Video\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our custom environment\n",
    "# Use importlib to ensure we get the latest version (clears cache)\n",
    "import importlib\n",
    "import prisoners_dilemma_env\n",
    "importlib.reload(prisoners_dilemma_env)\n",
    "\n",
    "from prisoners_dilemma_env import IteratedPrisonersDilemma, COOPERATE, DEFECT, ACTION_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59906ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# POLICY ITERATION RESULTS - MEMORY SCHEME 1\n",
      "################################################################################\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "OPPONENT: ALL-C (Always Cooperate)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-C\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -30.0000\n",
      "Max State Value: -30.0000\n",
      "Min State Value: -30.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "OPPONENT: ALL-D (Always Defect)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-D\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: 0.0000\n",
      "Max State Value: 0.0000\n",
      "Min State Value: 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "OPPONENT: TFT (Tit-for-Tat)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: TFT\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "(C, D)          1          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -10.0000  \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -10.0000  \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -12.0000\n",
      "Max State Value: -10.0000\n",
      "Min State Value: -14.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "OPPONENT: IMPERFECT-TFT (Imperfect Tit-for-Tat)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: IMPERFECT-TFT\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          0.0000     1.0000     Defect (D)      -17.2000  \n",
      "(C, D)          1          0.0000     1.0000     Defect (D)      -17.2000  \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -15.6000\n",
      "Max State Value: -14.0000\n",
      "Min State Value: -17.2000\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# END OF EXPERIMENT\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from operator import ge\n",
    "\n",
    "strategies = [\"ALL-C\", \"ALL-D\", \"TFT\", \"IMPERFECT-TFT\"]\n",
    "memory_schemes = [1, 2]\n",
    "gamma = 0.9\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT: Policy Iteration for Different Opponent Strategies\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# POLICY ITERATION RESULTS - MEMORY SCHEME 1\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Opponent: ALL-C (Always Cooperate)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"OPPONENT: ALL-C (Always Cooperate)\")\n",
    "print(\"-\" * 80)\n",
    "env = IteratedPrisonersDilemma(opponent_strategy=\"ALL-C\", memory_scheme=1)\n",
    "best_policy, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
    "print_policy(best_policy, value_function, env, opponent_strategy=\"ALL-C\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Opponent: ALL-D (Always Defect)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"OPPONENT: ALL-D (Always Defect)\")\n",
    "print(\"-\" * 80)\n",
    "env = IteratedPrisonersDilemma(opponent_strategy=\"ALL-D\", memory_scheme=1)\n",
    "best_policy, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
    "print_policy(best_policy, value_function, env, opponent_strategy=\"ALL-D\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Opponent: TFT (Tit-for-Tat)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"OPPONENT: TFT (Tit-for-Tat)\")\n",
    "print(\"-\" * 80)\n",
    "env = IteratedPrisonersDilemma(opponent_strategy=\"TFT\", memory_scheme=1)\n",
    "best_policy, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
    "print_policy(best_policy, value_function, env, opponent_strategy=\"TFT\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Opponent: IMPERFECT-TFT (Imperfect Tit-for-Tat)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"OPPONENT: IMPERFECT-TFT (Imperfect Tit-for-Tat)\")\n",
    "print(\"-\" * 80)\n",
    "env = IteratedPrisonersDilemma(opponent_strategy=\"IMPERFECT-TFT\", memory_scheme=1)\n",
    "best_policy, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
    "print_policy(best_policy, value_function, env, opponent_strategy=\"IMPERFECT-TFT\")\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# END OF EXPERIMENT\")\n",
    "print(\"#\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae73ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT: Policy Iteration for Different Discount Factors (Gamma)\n",
    "# ============================================================================\n",
    "# Define gamma values to test\n",
    "gamma_values = [0.1, 0.5, 0.9, 0.99]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a1966ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\n",
      "################################################################################\n",
      "# Opponent Strategy: ALL-C (Always Cooperate)\n",
      "# Memory Scheme: 1\n",
      "################################################################################\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-C\n",
      "Discount Factor (γ): 0.1\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -3.3333   \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -3.3333   \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   -3.3333   \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   -3.3333   \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -3.3333\n",
      "Max State Value: -3.3333\n",
      "Min State Value: -3.3333\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-C\n",
      "Discount Factor (γ): 0.5\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -6.0000   \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -6.0000   \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   -6.0000   \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   -6.0000   \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -6.0000\n",
      "Max State Value: -6.0000\n",
      "Min State Value: -6.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.9\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-C\n",
      "Discount Factor (γ): 0.9\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -30.0000\n",
      "Max State Value: -30.0000\n",
      "Min State Value: -30.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.99\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-C\n",
      "Discount Factor (γ): 0.99\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -299.9999 \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -299.9999 \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   -299.9999 \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   -299.9999 \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -299.9999\n",
      "Max State Value: -299.9999\n",
      "Min State Value: -299.9999\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# END OF EXPERIMENT - ALL-C\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPPONENT: ALL-C (Always Cooperate) - Different Discount Factors\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\")\n",
    "print(\"#\" * 80)\n",
    "print(\"# Opponent Strategy: ALL-C (Always Cooperate)\")\n",
    "print(\"# Memory Scheme: 1\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(f\"DISCOUNT FACTOR (γ): {gamma}\")\n",
    "    print(\"-\" * 80)\n",
    "    env = IteratedPrisonersDilemma(opponent_strategy=\"ALL-C\", memory_scheme=1)\n",
    "    best_policy, value_function = env.policy_iteration(gamma=gamma, theta=0.000001, max_iterations=100)\n",
    "    print_policy(best_policy, value_function, env, opponent_strategy=\"ALL-C\", gamma=gamma)\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# END OF EXPERIMENT - ALL-C\")\n",
    "print(\"#\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90eeb220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\n",
      "################################################################################\n",
      "# Opponent Strategy: ALL-D (Always Defect)\n",
      "# Memory Scheme: 1\n",
      "################################################################################\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-D\n",
      "Discount Factor (γ): 0.1\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: 0.0000\n",
      "Max State Value: 0.0000\n",
      "Min State Value: 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-D\n",
      "Discount Factor (γ): 0.5\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: 0.0000\n",
      "Max State Value: 0.0000\n",
      "Min State Value: 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.9\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-D\n",
      "Discount Factor (γ): 0.9\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: 0.0000\n",
      "Max State Value: 0.0000\n",
      "Min State Value: 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.99\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-D\n",
      "Discount Factor (γ): 0.99\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: 0.0000\n",
      "Max State Value: 0.0000\n",
      "Min State Value: 0.0000\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# END OF EXPERIMENT - ALL-D\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPPONENT: ALL-D (Always Defect) - Different Discount Factors\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\")\n",
    "print(\"#\" * 80)\n",
    "print(\"# Opponent Strategy: ALL-D (Always Defect)\")\n",
    "print(\"# Memory Scheme: 1\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(f\"DISCOUNT FACTOR (γ): {gamma}\")\n",
    "    print(\"-\" * 80)\n",
    "    env = IteratedPrisonersDilemma(opponent_strategy=\"ALL-D\", memory_scheme=1)\n",
    "    best_policy, value_function = env.policy_iteration(gamma=gamma, theta=0.000001, max_iterations=100)\n",
    "    print_policy(best_policy, value_function, env, opponent_strategy=\"ALL-D\", gamma=gamma)\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# END OF EXPERIMENT - ALL-D\")\n",
    "print(\"#\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2011e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\n",
      "################################################################################\n",
      "# Opponent Strategy: TFT (Tit-for-Tat)\n",
      "# Memory Scheme: 1\n",
      "################################################################################\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: TFT\n",
      "Discount Factor (γ): 0.1\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -3.3333   \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -3.3333   \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   -0.3333   \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   -0.3333   \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -1.8333\n",
      "Max State Value: -0.3333\n",
      "Min State Value: -3.3333\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: TFT\n",
      "Discount Factor (γ): 0.5\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -6.0000   \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -6.0000   \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -2.0000   \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -2.0000   \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -4.0000\n",
      "Max State Value: -2.0000\n",
      "Min State Value: -6.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.9\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: TFT\n",
      "Discount Factor (γ): 0.9\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "(C, D)          1          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -10.0000  \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -10.0000  \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -12.0000\n",
      "Max State Value: -10.0000\n",
      "Min State Value: -14.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.99\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: TFT\n",
      "Discount Factor (γ): 0.99\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          0.0000     1.0000     Defect (D)      -103.9999 \n",
      "(C, D)          1          0.0000     1.0000     Defect (D)      -103.9999 \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -99.9999  \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -99.9999  \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -101.9999\n",
      "Max State Value: -99.9999\n",
      "Min State Value: -103.9999\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# END OF EXPERIMENT - TFT\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPPONENT: TFT (Tit-for-Tat) - Different Discount Factors\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\")\n",
    "print(\"#\" * 80)\n",
    "print(\"# Opponent Strategy: TFT (Tit-for-Tat)\")\n",
    "print(\"# Memory Scheme: 1\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(f\"DISCOUNT FACTOR (γ): {gamma}\")\n",
    "    print(\"-\" * 80)\n",
    "    env = IteratedPrisonersDilemma(opponent_strategy=\"TFT\", memory_scheme=1)\n",
    "    best_policy, value_function = env.policy_iteration(gamma=gamma, theta=0.000001, max_iterations=100)\n",
    "    print_policy(best_policy, value_function, env, opponent_strategy=\"TFT\", gamma=gamma)\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# END OF EXPERIMENT - TFT\")\n",
    "print(\"#\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "858ca6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\n",
      "################################################################################\n",
      "# Opponent Strategy: IMPERFECT-TFT (Imperfect Tit-for-Tat)\n",
      "# Memory Scheme: 1\n",
      "################################################################################\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: IMPERFECT-TFT\n",
      "Discount Factor (γ): 0.1\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -3.0000   \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -3.0000   \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   -0.6000   \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   -0.6000   \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -1.8000\n",
      "Max State Value: -0.6000\n",
      "Min State Value: -3.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: IMPERFECT-TFT\n",
      "Discount Factor (γ): 0.5\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -5.4000   \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -5.4000   \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -2.8000   \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -2.8000   \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -4.1000\n",
      "Max State Value: -2.8000\n",
      "Min State Value: -5.4000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.9\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: IMPERFECT-TFT\n",
      "Discount Factor (γ): 0.9\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          0.0000     1.0000     Defect (D)      -17.2000  \n",
      "(C, D)          1          0.0000     1.0000     Defect (D)      -17.2000  \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -15.6000\n",
      "Max State Value: -14.0000\n",
      "Min State Value: -17.2000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DISCOUNT FACTOR (γ): 0.99\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: IMPERFECT-TFT\n",
      "Discount Factor (γ): 0.99\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          0.0000     1.0000     Defect (D)      -143.1999 \n",
      "(C, D)          1          0.0000     1.0000     Defect (D)      -143.1999 \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -139.9999 \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -139.9999 \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -141.5999\n",
      "Max State Value: -139.9999\n",
      "Min State Value: -143.1999\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# END OF EXPERIMENT - IMPERFECT-TFT\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPPONENT: IMPERFECT-TFT (Imperfect Tit-for-Tat) - Different Discount Factors\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\")\n",
    "print(\"#\" * 80)\n",
    "print(\"# Opponent Strategy: IMPERFECT-TFT (Imperfect Tit-for-Tat)\")\n",
    "print(\"# Memory Scheme: 1\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(f\"DISCOUNT FACTOR (γ): {gamma}\")\n",
    "    print(\"-\" * 80)\n",
    "    env = IteratedPrisonersDilemma(opponent_strategy=\"IMPERFECT-TFT\", memory_scheme=1)\n",
    "    best_policy, value_function = env.policy_iteration(gamma=gamma, theta=0.000001, max_iterations=100)\n",
    "    print_policy(best_policy, value_function, env, opponent_strategy=\"IMPERFECT-TFT\", gamma=gamma)\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# END OF EXPERIMENT - IMPERFECT-TFT\")\n",
    "print(\"#\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d6b794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# TESTING play_policy METHOD\n",
      "################################################################################\n",
      "\n",
      "Opponent Strategy: TFT\n",
      "Memory Scheme: 1\n",
      "Policy shape: (4, 2)\n",
      "\n",
      "================================================================================\n",
      "CUMULATIVE RESULTS\n",
      "================================================================================\n",
      "Cumulative Reward: -54.00\n",
      "Number of Steps: 50\n",
      "Average Reward per Step: -1.08\n",
      "\n",
      "First 10 Steps:\n",
      "Step   Agent    Opponent   Reward    \n",
      "----------------------------------------\n",
      "1      D        C          -5.00     \n",
      "2      D        D          -1.00     \n",
      "3      D        D          -1.00     \n",
      "4      D        D          -1.00     \n",
      "5      D        D          -1.00     \n",
      "6      D        D          -1.00     \n",
      "7      D        D          -1.00     \n",
      "8      D        D          -1.00     \n",
      "9      D        D          -1.00     \n",
      "10     D        D          -1.00     \n",
      "\n",
      "...\n",
      "\n",
      "Last 10 Steps:\n",
      "Step   Agent    Opponent   Reward    \n",
      "----------------------------------------\n",
      "41     D        D          -1.00     \n",
      "42     D        D          -1.00     \n",
      "43     D        D          -1.00     \n",
      "44     D        D          -1.00     \n",
      "45     D        D          -1.00     \n",
      "46     D        D          -1.00     \n",
      "47     D        D          -1.00     \n",
      "48     D        D          -1.00     \n",
      "49     D        D          -1.00     \n",
      "50     D        D          -1.00     \n",
      "\n",
      "################################################################################\n",
      "# END OF TEST\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
