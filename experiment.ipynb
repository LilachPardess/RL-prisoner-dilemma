{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "402d89db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy, value_function, env, opponent_strategy=None):\n",
    "    \"\"\"\n",
    "    Print policy and value function in a readable format.\n",
    "    \n",
    "    Args:\n",
    "        policy: Policy matrix of shape (num_states, num_actions)\n",
    "        value_function: Value function array of shape (num_states,)\n",
    "        env: IteratedPrisonersDilemma environment instance\n",
    "        opponent_strategy: Optional string to display opponent strategy name\n",
    "    \"\"\"\n",
    "    num_states = env.observation_space.n\n",
    "    memory_scheme = env.memory_scheme\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"OPTIMAL POLICY AND VALUE FUNCTION\")\n",
    "    print(\"=\" * 80)\n",
    "    if opponent_strategy:\n",
    "        print(f\"Opponent Strategy: {opponent_strategy}\")\n",
    "    print(f\"Memory Scheme: {memory_scheme}\")\n",
    "    print(f\"Number of States: {num_states}\")\n",
    "    print()\n",
    "    \n",
    "    # Print policy table\n",
    "    print(\"POLICY (π):\")\n",
    "    print(\"-\" * 80)\n",
    "    if memory_scheme == 1:\n",
    "        # Memory-1: 4 states\n",
    "        state_names = [\"(C, C)\", \"(C, D)\", \"(D, C)\", \"(D, D)\"]\n",
    "        print(f\"{'State':<15} {'State ID':<10} {'P(C)':<10} {'P(D)':<10} {'Best Action':<15} {'V(s)':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        for s in range(num_states):\n",
    "            state_name = state_names[s]\n",
    "            p_cooperate = policy[s, 0]\n",
    "            p_defect = policy[s, 1]\n",
    "            best_action_idx = np.argmax(policy[s])\n",
    "            best_action = \"Cooperate (C)\" if best_action_idx == 0 else \"Defect (D)\"\n",
    "            v_value = value_function[s]\n",
    "            print(f\"{state_name:<15} {s:<10} {p_cooperate:<10.4f} {p_defect:<10.4f} {best_action:<15} {v_value:<10.4f}\")\n",
    "    else:\n",
    "        # Memory-2: 16 states\n",
    "        print(f\"{'State':<25} {'State ID':<10} {'P(C)':<10} {'P(D)':<10} {'Best Action':<15} {'V(s)':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        for s in range(num_states):\n",
    "            # Decode state: [A_t-1, O_t-1, A_t-2, O_t-2]\n",
    "            state_vector = []\n",
    "            temp = s\n",
    "            for i in range(4):\n",
    "                bit = temp % 2\n",
    "                state_vector.insert(0, bit)\n",
    "                temp = temp // 2\n",
    "            A_t1, O_t1, A_t2, O_t2 = state_vector\n",
    "            \n",
    "            # Format state name\n",
    "            action_map = {0: \"C\", 1: \"D\"}\n",
    "            state_name = f\"({action_map[A_t1]},{action_map[O_t1]})→({action_map[A_t2]},{action_map[O_t2]})\"\n",
    "            \n",
    "            p_cooperate = policy[s, 0]\n",
    "            p_defect = policy[s, 1]\n",
    "            best_action_idx = np.argmax(policy[s])\n",
    "            best_action = \"Cooperate (C)\" if best_action_idx == 0 else \"Defect (D)\"\n",
    "            v_value = value_function[s]\n",
    "            print(f\"{state_name:<25} {s:<10} {p_cooperate:<10.4f} {p_defect:<10.4f} {best_action:<15} {v_value:<10.4f}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Average State Value: {np.mean(value_function):.4f}\")\n",
    "    print(f\"Max State Value: {np.max(value_function):.4f}\")\n",
    "    print(f\"Min State Value: {np.min(value_function):.4f}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae8652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best policy for each opponent type and discount factor.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bad8d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from IPython.display import Video\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our custom environment\n",
    "# Use importlib to ensure we get the latest version (clears cache)\n",
    "import importlib\n",
    "import prisoners_dilemma_env\n",
    "importlib.reload(prisoners_dilemma_env)\n",
    "\n",
    "from prisoners_dilemma_env import IteratedPrisonersDilemma, COOPERATE, DEFECT, ACTION_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59906ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# POLICY ITERATION RESULTS - MEMORY SCHEME 1\n",
      "################################################################################\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "OPPONENT: ALL-C (Always Cooperate)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-C\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   -30.0000  \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -30.0000\n",
      "Max State Value: -30.0000\n",
      "Min State Value: -30.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "OPPONENT: ALL-D (Always Defect)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: ALL-D\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(C, D)          1          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, C)          2          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "(D, D)          3          1.0000     0.0000     Cooperate (C)   0.0000    \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: 0.0000\n",
      "Max State Value: 0.0000\n",
      "Min State Value: 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "OPPONENT: TFT (Tit-for-Tat)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: TFT\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "(C, D)          1          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -10.0000  \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -10.0000  \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -12.0000\n",
      "Max State Value: -10.0000\n",
      "Min State Value: -14.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "OPPONENT: IMPERFECT-TFT (Imperfect Tit-for-Tat)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "OPTIMAL POLICY AND VALUE FUNCTION\n",
      "================================================================================\n",
      "Opponent Strategy: IMPERFECT-TFT\n",
      "Memory Scheme: 1\n",
      "Number of States: 4\n",
      "\n",
      "POLICY (π):\n",
      "--------------------------------------------------------------------------------\n",
      "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
      "--------------------------------------------------------------------------------\n",
      "(C, C)          0          0.0000     1.0000     Defect (D)      -17.2000  \n",
      "(C, D)          1          0.0000     1.0000     Defect (D)      -17.2000  \n",
      "(D, C)          2          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "(D, D)          3          0.0000     1.0000     Defect (D)      -14.0000  \n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Average State Value: -15.6000\n",
      "Max State Value: -14.0000\n",
      "Min State Value: -17.2000\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# END OF EXPERIMENT\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from operator import ge\n",
    "\n",
    "strategies = [\"ALL-C\", \"ALL-D\", \"TFT\", \"IMPERFECT-TFT\"]\n",
    "memory_schemes = [1, 2]\n",
    "gamma = 0.9\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT: Policy Iteration for Different Opponent Strategies\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# POLICY ITERATION RESULTS - MEMORY SCHEME 1\")\n",
    "print(\"#\" * 80)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Opponent: ALL-C (Always Cooperate)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"OPPONENT: ALL-C (Always Cooperate)\")\n",
    "print(\"-\" * 80)\n",
    "env = IteratedPrisonersDilemma(opponent_strategy=\"ALL-C\", memory_scheme=1)\n",
    "best_policy, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
    "print_policy(best_policy, value_function, env, opponent_strategy=\"ALL-C\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Opponent: ALL-D (Always Defect)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"OPPONENT: ALL-D (Always Defect)\")\n",
    "print(\"-\" * 80)\n",
    "env = IteratedPrisonersDilemma(opponent_strategy=\"ALL-D\", memory_scheme=1)\n",
    "best_policy, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
    "print_policy(best_policy, value_function, env, opponent_strategy=\"ALL-D\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Opponent: TFT (Tit-for-Tat)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"OPPONENT: TFT (Tit-for-Tat)\")\n",
    "print(\"-\" * 80)\n",
    "env = IteratedPrisonersDilemma(opponent_strategy=\"TFT\", memory_scheme=1)\n",
    "best_policy, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
    "print_policy(best_policy, value_function, env, opponent_strategy=\"TFT\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Opponent: IMPERFECT-TFT (Imperfect Tit-for-Tat)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"OPPONENT: IMPERFECT-TFT (Imperfect Tit-for-Tat)\")\n",
    "print(\"-\" * 80)\n",
    "env = IteratedPrisonersDilemma(opponent_strategy=\"IMPERFECT-TFT\", memory_scheme=1)\n",
    "best_policy, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
    "print_policy(best_policy, value_function, env, opponent_strategy=\"IMPERFECT-TFT\")\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"# END OF EXPERIMENT\")\n",
    "print(\"#\" * 80 + \"\\n\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
