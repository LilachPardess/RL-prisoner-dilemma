{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "86b2a106",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "LOCAL JUPYTER ENVIRONMENT DETECTED\n",
            "================================================================================\n",
            "Make sure prisoners_dilemma_env.py and utils.py are in the same directory.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# GOOGLE COLAB SETUP\n",
        "# ============================================================================\n",
        "# Run this cell first when using Google Colab\n",
        "# It will prompt you to upload the required Python files\n",
        "\n",
        "# Install required packages\n",
        "import subprocess\n",
        "import sys\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \n",
        "                       \"gymnasium\", \"numpy\", \"matplotlib\", \"seaborn\", \n",
        "                       \"tqdm\", \"pandas\", \"imageio\", \"imageio-ffmpeg\", \n",
        "                       \"pillow\", \"moviepy\"])\n",
        "\n",
        "# Upload required files (only in Colab)\n",
        "try:\n",
        "    from google.colab import files  # type: ignore\n",
        "    print(\"=\" * 80)\n",
        "    print(\"GOOGLE COLAB DETECTED\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"\\nPlease upload the following files:\")\n",
        "    print(\"1. prisoners_dilemma_env.py\")\n",
        "    print(\"2. utils.py\")\n",
        "    uploaded = files.upload()\n",
        "    \n",
        "    # Verify files were uploaded\n",
        "    import os\n",
        "    for filename in uploaded.keys():\n",
        "        print(f\"✓ Uploaded: {filename}\")\n",
        "except ImportError:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"LOCAL JUPYTER ENVIRONMENT DETECTED\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Make sure prisoners_dilemma_env.py and utils.py are in the same directory.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "d2ae8652",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Find the best policy for each opponent type and discount factor.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "bad8d6e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary libraries\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from IPython.display import Video\n",
        "import seaborn as sns\n",
        "\n",
        "# Import our custom environment\n",
        "# Use importlib to ensure we get the latest version (clears cache)\n",
        "import importlib\n",
        "import prisoners_dilemma_env\n",
        "importlib.reload(prisoners_dilemma_env)\n",
        "\n",
        "from prisoners_dilemma_env import IteratedPrisonersDilemma, COOPERATE, DEFECT, ACTION_MAP\n",
        "\n",
        "# Import utility functions\n",
        "import utils\n",
        "importlib.reload(utils)\n",
        "from utils import print_policy, print_comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "59906ee0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "OPTIMAL POLICY AND VALUE FUNCTION\n",
            "================================================================================\n",
            "Opponent Strategy: ALL-C\n",
            "Memory Scheme: 1\n",
            "\n",
            "POLICY (π):\n",
            "--------------------------------------------------------------------------------\n",
            "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
            "--------------------------------------------------------------------------------\n",
            "(C, C)          0          0.0000     1.0000     Defect (D)      50.0000   \n",
            "(C, D)          1          0.0000     1.0000     Defect (D)      50.0000   \n",
            "(D, C)          2          0.0000     1.0000     Defect (D)      50.0000   \n",
            "(D, D)          3          0.0000     1.0000     Defect (D)      50.0000   \n",
            "\n",
            "\n",
            "Random vs Best Policy:\n",
            "  Random: 400.00\n",
            "  Best:   500.00\n",
            "  Diff:   100.00\n",
            "\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL POLICY AND VALUE FUNCTION\n",
            "================================================================================\n",
            "Opponent Strategy: ALL-D\n",
            "Memory Scheme: 1\n",
            "\n",
            "POLICY (π):\n",
            "--------------------------------------------------------------------------------\n",
            "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
            "--------------------------------------------------------------------------------\n",
            "(C, C)          0          0.0000     1.0000     Defect (D)      10.0000   \n",
            "(C, D)          1          0.0000     1.0000     Defect (D)      10.0000   \n",
            "(D, C)          2          0.0000     1.0000     Defect (D)      10.0000   \n",
            "(D, D)          3          0.0000     1.0000     Defect (D)      10.0000   \n",
            "\n",
            "\n",
            "Random vs Best Policy:\n",
            "  Random: 44.00\n",
            "  Best:   100.00\n",
            "  Diff:   56.00\n",
            "\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL POLICY AND VALUE FUNCTION\n",
            "================================================================================\n",
            "Opponent Strategy: TFT\n",
            "Memory Scheme: 1\n",
            "\n",
            "POLICY (π):\n",
            "--------------------------------------------------------------------------------\n",
            "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
            "--------------------------------------------------------------------------------\n",
            "(C, C)          0          1.0000     0.0000     Cooperate (C)   30.0000   \n",
            "(C, D)          1          1.0000     0.0000     Cooperate (C)   30.0000   \n",
            "(D, C)          2          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(D, D)          3          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "\n",
            "\n",
            "Random vs Best Policy:\n",
            "  Random: 235.00\n",
            "  Best:   300.00\n",
            "  Diff:   65.00\n",
            "\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL POLICY AND VALUE FUNCTION\n",
            "================================================================================\n",
            "Opponent Strategy: TFT\n",
            "Memory Scheme: 2\n",
            "\n",
            "POLICY (π):\n",
            "--------------------------------------------------------------------------------\n",
            "State                     State ID   P(C)       P(D)       Best Action     V(s)      \n",
            "--------------------------------------------------------------------------------\n",
            "(C,C)→(C,C)               0          1.0000     0.0000     Cooperate (C)   30.0000   \n",
            "(C,C)→(C,D)               1          1.0000     0.0000     Cooperate (C)   30.0000   \n",
            "(C,C)→(D,C)               2          1.0000     0.0000     Cooperate (C)   30.0000   \n",
            "(C,C)→(D,D)               3          1.0000     0.0000     Cooperate (C)   30.0000   \n",
            "(C,D)→(C,C)               4          1.0000     0.0000     Cooperate (C)   30.0000   \n",
            "(C,D)→(C,D)               5          1.0000     0.0000     Cooperate (C)   30.0000   \n",
            "(C,D)→(D,C)               6          1.0000     0.0000     Cooperate (C)   30.0000   \n",
            "(C,D)→(D,D)               7          1.0000     0.0000     Cooperate (C)   30.0000   \n",
            "(D,C)→(C,C)               8          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(D,C)→(C,D)               9          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(D,C)→(D,C)               10         1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(D,C)→(D,D)               11         1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(D,D)→(C,C)               12         1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(D,D)→(C,D)               13         1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(D,D)→(D,C)               14         1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(D,D)→(D,D)               15         1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "\n",
            "\n",
            "Random vs Best Policy:\n",
            "  Random: 233.00\n",
            "  Best:   300.00\n",
            "  Diff:   67.00\n",
            "\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL POLICY AND VALUE FUNCTION\n",
            "================================================================================\n",
            "Opponent Strategy: IMPERFECT-TFT\n",
            "Memory Scheme: 1\n",
            "\n",
            "POLICY (π):\n",
            "--------------------------------------------------------------------------------\n",
            "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
            "--------------------------------------------------------------------------------\n",
            "(C, C)          0          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(C, D)          1          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(D, C)          2          1.0000     0.0000     Cooperate (C)   24.6000   \n",
            "(D, D)          3          1.0000     0.0000     Cooperate (C)   24.6000   \n",
            "\n",
            "\n",
            "Random vs Best Policy:\n",
            "  Random: 226.00\n",
            "  Best:   288.00\n",
            "  Diff:   62.00\n",
            "\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL POLICY AND VALUE FUNCTION\n",
            "================================================================================\n",
            "Opponent Strategy: IMPERFECT-TFT\n",
            "Memory Scheme: 2\n",
            "\n",
            "POLICY (π):\n",
            "--------------------------------------------------------------------------------\n",
            "State                     State ID   P(C)       P(D)       Best Action     V(s)      \n",
            "--------------------------------------------------------------------------------\n",
            "(C,C)→(C,C)               0          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(C,C)→(C,D)               1          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(C,C)→(D,C)               2          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(C,C)→(D,D)               3          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(C,D)→(C,C)               4          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(C,D)→(C,D)               5          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(C,D)→(D,C)               6          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(C,D)→(D,D)               7          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(D,C)→(C,C)               8          1.0000     0.0000     Cooperate (C)   24.6000   \n",
            "(D,C)→(C,D)               9          1.0000     0.0000     Cooperate (C)   24.6000   \n",
            "(D,C)→(D,C)               10         1.0000     0.0000     Cooperate (C)   24.6000   \n",
            "(D,C)→(D,D)               11         1.0000     0.0000     Cooperate (C)   24.6000   \n",
            "(D,D)→(C,C)               12         1.0000     0.0000     Cooperate (C)   24.6000   \n",
            "(D,D)→(C,D)               13         1.0000     0.0000     Cooperate (C)   24.6000   \n",
            "(D,D)→(D,C)               14         1.0000     0.0000     Cooperate (C)   24.6000   \n",
            "(D,D)→(D,D)               15         1.0000     0.0000     Cooperate (C)   24.6000   \n",
            "\n",
            "\n",
            "Random vs Best Policy:\n",
            "  Random: 214.00\n",
            "  Best:   273.00\n",
            "  Diff:   59.00\n",
            "\n",
            "\n",
            "################################################################################\n",
            "# END OF EXPERIMENT\n",
            "################################################################################\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from operator import ge\n",
        "\n",
        "strategies = [\"ALL-C\", \"ALL-D\", \"TFT\", \"IMPERFECT-TFT\"]\n",
        "memory_schemes = [1, 2]\n",
        "gamma = 0.9\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENT: Policy Iteration for Different Opponent Strategies\n",
        "# ============================================================================\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Opponent: ALL-C (Always Cooperate)\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "env = IteratedPrisonersDilemma(opponent_strategy=\"ALL-C\", memory_scheme=1)\n",
        "best_policy_all_c, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
        "print_policy(best_policy_all_c, value_function, env, opponent_strategy=\"ALL-C\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# COMPARISON: Random Policy vs Best Policy for ALL-C\n",
        "# ----------------------------------------------------------------------------\n",
        "comparison = env.test_against_random(best_policy_all_c, num_steps=100)\n",
        "print_comparison(comparison)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Opponent: ALL-D (Always Defect)\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "env = IteratedPrisonersDilemma(opponent_strategy=\"ALL-D\", memory_scheme=1)\n",
        "best_policy_all_d, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
        "print_policy(best_policy_all_d, value_function, env, opponent_strategy=\"ALL-D\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# COMPARISON: Random Policy vs Best Policy for ALL-D\n",
        "# ----------------------------------------------------------------------------\n",
        "comparison = env.test_against_random(best_policy_all_d, num_steps=100)\n",
        "print_comparison(comparison)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Opponent: TFT (Tit-for-Tat)\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "env = IteratedPrisonersDilemma(opponent_strategy=\"TFT\", memory_scheme=1)\n",
        "best_policy_tft, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
        "print_policy(best_policy_tft, value_function, env, opponent_strategy=\"TFT\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# COMPARISON: Random Policy vs Best Policy for TFT\n",
        "# ----------------------------------------------------------------------------\n",
        "comparison = env.test_against_random(best_policy_tft, num_steps=100)\n",
        "print_comparison(comparison)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Opponent: Memory 2 TFT (Tit-for-Tat)\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "env = IteratedPrisonersDilemma(opponent_strategy=\"TFT\", memory_scheme=2)\n",
        "best_policy_tft_m2, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
        "print_policy(best_policy_tft_m2, value_function, env, opponent_strategy=\"TFT\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# COMPARISON: Random Policy vs Best Policy for TFT\n",
        "# ----------------------------------------------------------------------------\n",
        "comparison = env.test_against_random(best_policy_tft_m2, num_steps=100)\n",
        "print_comparison(comparison)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Opponent: IMPERFECT-TFT (Imperfect Tit-for-Tat)\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "env = IteratedPrisonersDilemma(opponent_strategy=\"IMPERFECT-TFT\", memory_scheme=1)\n",
        "best_policy_imperfect_tft, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
        "print_policy(best_policy_imperfect_tft, value_function, env, opponent_strategy=\"IMPERFECT-TFT\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# COMPARISON: Random Policy vs Best Policy for IMPERFECT-TFT\n",
        "# ----------------------------------------------------------------------------\n",
        "comparison = env.test_against_random(best_policy_imperfect_tft, num_steps=100)\n",
        "print_comparison(comparison)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Memory 2 Opponent: IMPERFECT-TFT (Imperfect Tit-for-Tat)\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "env = IteratedPrisonersDilemma(opponent_strategy=\"IMPERFECT-TFT\", memory_scheme=2)\n",
        "best_policy_imperfect_tft_m2, value_function = env.policy_iteration(theta=0.000001, max_iterations=100)\n",
        "print_policy(best_policy_imperfect_tft_m2, value_function, env, opponent_strategy=\"IMPERFECT-TFT\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# COMPARISON: Random Policy vs Best Policy for IMPERFECT-TFT\n",
        "# ----------------------------------------------------------------------------\n",
        "comparison = env.test_against_random(best_policy_imperfect_tft_m2, num_steps=100)\n",
        "print_comparison(comparison)\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"#\" * 80)\n",
        "print(\"# END OF EXPERIMENT\")\n",
        "print(\"#\" * 80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "ae73ce10",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXPERIMENT: Policy Iteration for Different Discount Factors (Gamma)\n",
        "# ============================================================================\n",
        "# Define gamma values to test\n",
        "gamma_values = [0.1, 0.5, 0.9, 0.99]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "b90cd121",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "################################################################################\n",
            "# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\n",
            "################################################################################\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL POLICY AND VALUE FUNCTION\n",
            "================================================================================\n",
            "Opponent Strategy: TFT\n",
            "Discount Factor (γ): 0.1\n",
            "Memory Scheme: 1\n",
            "\n",
            "POLICY (π):\n",
            "--------------------------------------------------------------------------------\n",
            "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
            "--------------------------------------------------------------------------------\n",
            "(C, C)          0          0.0000     1.0000     Defect (D)      5.1111    \n",
            "(C, D)          1          0.0000     1.0000     Defect (D)      5.1111    \n",
            "(D, C)          2          0.0000     1.0000     Defect (D)      1.1111    \n",
            "(D, D)          3          0.0000     1.0000     Defect (D)      1.1111    \n",
            "\n",
            "\n",
            "Random vs Best Policy:\n",
            "  Random: 229.00\n",
            "  Best:   104.00\n",
            "  Diff:   -125.00\n",
            "\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL POLICY AND VALUE FUNCTION\n",
            "================================================================================\n",
            "Opponent Strategy: TFT\n",
            "Discount Factor (γ): 0.5\n",
            "Memory Scheme: 1\n",
            "\n",
            "POLICY (π):\n",
            "--------------------------------------------------------------------------------\n",
            "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
            "--------------------------------------------------------------------------------\n",
            "(C, C)          0          0.0000     1.0000     Defect (D)      6.6667    \n",
            "(C, D)          1          0.0000     1.0000     Defect (D)      6.6667    \n",
            "(D, C)          2          1.0000     0.0000     Cooperate (C)   3.3333    \n",
            "(D, D)          3          1.0000     0.0000     Cooperate (C)   3.3333    \n",
            "\n",
            "\n",
            "Random vs Best Policy:\n",
            "  Random: 235.00\n",
            "  Best:   250.00\n",
            "  Diff:   15.00\n",
            "\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL POLICY AND VALUE FUNCTION\n",
            "================================================================================\n",
            "Opponent Strategy: TFT\n",
            "Discount Factor (γ): 0.9\n",
            "Memory Scheme: 1\n",
            "\n",
            "POLICY (π):\n",
            "--------------------------------------------------------------------------------\n",
            "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
            "--------------------------------------------------------------------------------\n",
            "(C, C)          0          1.0000     0.0000     Cooperate (C)   30.0000   \n",
            "(C, D)          1          1.0000     0.0000     Cooperate (C)   30.0000   \n",
            "(D, C)          2          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(D, D)          3          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "\n",
            "\n",
            "Random vs Best Policy:\n",
            "  Random: 217.00\n",
            "  Best:   300.00\n",
            "  Diff:   83.00\n",
            "\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL POLICY AND VALUE FUNCTION\n",
            "================================================================================\n",
            "Opponent Strategy: TFT\n",
            "Discount Factor (γ): 0.99\n",
            "Memory Scheme: 1\n",
            "\n",
            "POLICY (π):\n",
            "--------------------------------------------------------------------------------\n",
            "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
            "--------------------------------------------------------------------------------\n",
            "(C, C)          0          1.0000     0.0000     Cooperate (C)   299.9999  \n",
            "(C, D)          1          1.0000     0.0000     Cooperate (C)   299.9999  \n",
            "(D, C)          2          1.0000     0.0000     Cooperate (C)   296.9999  \n",
            "(D, D)          3          1.0000     0.0000     Cooperate (C)   296.9999  \n",
            "\n",
            "\n",
            "Random vs Best Policy:\n",
            "  Random: 240.00\n",
            "  Best:   300.00\n",
            "  Diff:   60.00\n",
            "\n",
            "\n",
            "################################################################################\n",
            "# END OF EXPERIMENT - TFT\n",
            "################################################################################\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# OPPONENT: TFT (Tit-for-Tat) - Different Discount Factors\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"#\" * 80)\n",
        "print(\"# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\")\n",
        "print(\"#\" * 80)\n",
        "\n",
        "for gamma in gamma_values:\n",
        "    env = IteratedPrisonersDilemma(opponent_strategy=\"TFT\", memory_scheme=1)\n",
        "    best_policy, value_function = env.policy_iteration(gamma=gamma, theta=0.000001, max_iterations=100)\n",
        "    print_policy(best_policy, value_function, env, opponent_strategy=\"TFT\", gamma=gamma)\n",
        "    # ----------------------------------------------------------------------------\n",
        "    # COMPARISON: Random Policy vs Best Policy for TFT\n",
        "    # ----------------------------------------------------------------------------\n",
        "    comparison = env.test_against_random(best_policy, num_steps=100)\n",
        "    print_comparison(comparison)\n",
        "\n",
        "print(\"\\n\" + \"#\" * 80)\n",
        "print(\"# END OF EXPERIMENT - TFT\")\n",
        "print(\"#\" * 80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "4229fef1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "################################################################################\n",
            "# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\n",
            "################################################################################\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DISCOUNT FACTOR (γ): 0.1\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL POLICY AND VALUE FUNCTION\n",
            "================================================================================\n",
            "Opponent Strategy: IMPERFECT-TFT\n",
            "Discount Factor (γ): 0.1\n",
            "Memory Scheme: 1\n",
            "\n",
            "POLICY (π):\n",
            "--------------------------------------------------------------------------------\n",
            "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
            "--------------------------------------------------------------------------------\n",
            "(C, C)          0          0.0000     1.0000     Defect (D)      4.7556    \n",
            "(C, D)          1          0.0000     1.0000     Defect (D)      4.7556    \n",
            "(D, C)          2          0.0000     1.0000     Defect (D)      1.5556    \n",
            "(D, D)          3          0.0000     1.0000     Defect (D)      1.5556    \n",
            "\n",
            "\n",
            "Random vs Best Policy:\n",
            "  Random: 219.00\n",
            "  Best:   156.00\n",
            "  Diff:   -63.00\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DISCOUNT FACTOR (γ): 0.5\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL POLICY AND VALUE FUNCTION\n",
            "================================================================================\n",
            "Opponent Strategy: IMPERFECT-TFT\n",
            "Discount Factor (γ): 0.5\n",
            "Memory Scheme: 1\n",
            "\n",
            "POLICY (π):\n",
            "--------------------------------------------------------------------------------\n",
            "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
            "--------------------------------------------------------------------------------\n",
            "(C, C)          0          0.0000     1.0000     Defect (D)      6.3333    \n",
            "(C, D)          1          0.0000     1.0000     Defect (D)      6.3333    \n",
            "(D, C)          2          1.0000     0.0000     Cooperate (C)   3.4667    \n",
            "(D, D)          3          1.0000     0.0000     Cooperate (C)   3.4667    \n",
            "\n",
            "\n",
            "Random vs Best Policy:\n",
            "  Random: 256.00\n",
            "  Best:   250.00\n",
            "  Diff:   -6.00\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DISCOUNT FACTOR (γ): 0.9\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL POLICY AND VALUE FUNCTION\n",
            "================================================================================\n",
            "Opponent Strategy: IMPERFECT-TFT\n",
            "Discount Factor (γ): 0.9\n",
            "Memory Scheme: 1\n",
            "\n",
            "POLICY (π):\n",
            "--------------------------------------------------------------------------------\n",
            "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
            "--------------------------------------------------------------------------------\n",
            "(C, C)          0          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(C, D)          1          1.0000     0.0000     Cooperate (C)   27.0000   \n",
            "(D, C)          2          1.0000     0.0000     Cooperate (C)   24.6000   \n",
            "(D, D)          3          1.0000     0.0000     Cooperate (C)   24.6000   \n",
            "\n",
            "\n",
            "Random vs Best Policy:\n",
            "  Random: 224.00\n",
            "  Best:   264.00\n",
            "  Diff:   40.00\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DISCOUNT FACTOR (γ): 0.99\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "OPTIMAL POLICY AND VALUE FUNCTION\n",
            "================================================================================\n",
            "Opponent Strategy: IMPERFECT-TFT\n",
            "Discount Factor (γ): 0.99\n",
            "Memory Scheme: 1\n",
            "\n",
            "POLICY (π):\n",
            "--------------------------------------------------------------------------------\n",
            "State           State ID   P(C)       P(D)       Best Action     V(s)      \n",
            "--------------------------------------------------------------------------------\n",
            "(C, C)          0          1.0000     0.0000     Cooperate (C)   269.9999  \n",
            "(C, D)          1          1.0000     0.0000     Cooperate (C)   269.9999  \n",
            "(D, C)          2          1.0000     0.0000     Cooperate (C)   267.5999  \n",
            "(D, D)          3          1.0000     0.0000     Cooperate (C)   267.5999  \n",
            "\n",
            "\n",
            "Random vs Best Policy:\n",
            "  Random: 221.00\n",
            "  Best:   288.00\n",
            "  Diff:   67.00\n",
            "\n",
            "\n",
            "################################################################################\n",
            "# END OF EXPERIMENT - IMPERFECT-TFT\n",
            "################################################################################\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# OPPONENT: IMPERFECT-TFT (Imperfect Tit-for-Tat) - Different Discount Factors\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"#\" * 80)\n",
        "print(\"# POLICY ITERATION RESULTS - DIFFERENT DISCOUNT FACTORS (γ)\")\n",
        "print(\"#\" * 80)\n",
        "\n",
        "for gamma in gamma_values:\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(f\"DISCOUNT FACTOR (γ): {gamma}\")\n",
        "    print(\"-\" * 80)\n",
        "    env = IteratedPrisonersDilemma(opponent_strategy=\"IMPERFECT-TFT\", memory_scheme=1)\n",
        "    best_policy, value_function = env.policy_iteration(gamma=gamma, theta=0.000001, max_iterations=100)\n",
        "    print_policy(best_policy, value_function, env, opponent_strategy=\"IMPERFECT-TFT\", gamma=gamma)\n",
        "    # ----------------------------------------------------------------------------\n",
        "    # COMPARISON: Random Policy vs Best Policy for IMPERFECT-TFT\n",
        "    # ----------------------------------------------------------------------------\n",
        "    comparison = env.test_against_random(best_policy, num_steps=100)\n",
        "    print_comparison(comparison)\n",
        "\n",
        "print(\"\\n\" + \"#\" * 80)\n",
        "print(\"# END OF EXPERIMENT - IMPERFECT-TFT\")\n",
        "print(\"#\" * 80 + \"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
